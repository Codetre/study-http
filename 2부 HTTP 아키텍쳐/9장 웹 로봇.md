- 웹 로봇: 사람과 상호 작용을 하지 않고 연속된 웹 트랜잭션을 자동으로 수행하는 SW.

# 9.1 크롤러와 크롤링
- 웹 크롤러 기능: 문서가 가리키는 링크를 따라가면서 웹을 순회하는 로봇. '스파이더'라고 불리기도 한다.
- 웹 크롤러 역할: 인터넷 검색엔진이 사용할 데이터베이스 구축.

## 9.1.1 어디에서 시작하는가: '루트 집합'
- 루트 집합(root set): 크롤러의 시작 방문지들의 URL 집합.
    - 선별 기준: 관심 있는 웹페이지들이 충분히 DB에 포함될 수 있도록 다양한 곳을 고를 것.
    - 좋은 루트 집합의 조건:
        - 크고 인기 있는 웹사이트
        - 새로 생성된 페이지 목록
        - 자주 링크되지 않는 잘 알려져 있지 않은 페이지들

## 9.1.2 링크 추출과 상대 링크 정상화
- 크롤러가 방문지를 찾아내는 방법: 루트 집합 문서들의 링크 URL을 파싱 및 절대 링크로 변환하여 방문할 목록에 추가.

## 9.1.3 순환 피하기
- 주의점: 크롤러가 루프나 순환에 빠지지 않아야 한다.
    ```mermaid
    flowchart LR;
        A --> B;
        A --> D;
        B --> C;
        C --> A;
        E --> B;
        linkStyle 0 stroke:#ff3,stroke-width:4px,color:red;
        linkStyle 2 stroke:#ff3,stroke-width:4px,color:red;
        linkStyle 3 stroke:#ff3,stroke-width:4px,color:red;
    ```
    - A, B, C 세 문서를 끊임 없이 클로러는 수집하게 된다.

## 9.1.4 루프와 중복
- 순환이 해로운 이유:
    - 크롤러를 무한 수집 루프에 빠지게 만든다.
    - 루프는 웹 서버가 끊임 없는 요청을 받게 만들어 부하를 높인다. 이는 다른 요청을 처리할 수 없게 만들 수 있다.
    - 중복 컨텐츠(dups)로 DB가 채워지게 된다. 

## 9.1.5 빵 부스러기의 흔적
- 워낙 웹이 방대해서 방문했던 곳을 추적하기는 어렵다. 
- URL 방문을 추적하려면:
    - 복잡한 자료 구조를 사용할 필요가 있다.
    - 방대한 DB를 위해 많은 공간을 확보해야 한다.
- 관리 기법에는:
    - 트리나 해시 테이블 같은 자료 구조[^참고 자료].
    - 느슨한 존재 비트맵: 
        1. 해시 함수로 URL을 고정된 크기의 해시값으로 변환한다.
        2. 존재 비트 배열(presence bit array)로 해시값을 채운다.
        3. 크롤러가 URL이 가리키는 어떤 문서를 크롤링하면 존재 비트 배열 중 일치하는 것이 있는지 검사한다.
    - 체크포인트: 로봇이 갑자기 중단되는 경우를 대비하여 URL 목록을 디스크에 저장.
    - 파티셔닝: 
        - 배경: 한 대의 기기에서 하나의 크롤러만이 작업하기에는 분량이 많다.
        - 역할: 병렬화로 여러 기기에 여러 크롤러를 돌린다. 각 크롤러에게 수집할 분량을 나눠 준다.

[^참고 자료]: Managing Gigabytes: Compressing and Indexing Documents and Images - Witten

## 9.1.6 별칭과 로봇 순환
- URL 탐색과 관리를 어렵게 만드는 또 다른 요인: URL은 별칭(alias)을 지닐 수 있다. 같은 리소스, 다른 URL. 
    ![[IMG_4100.jpg]]

## 9.1.7 URL 정규화하기
- 별칭 문제 해결 수단: URL 표준 형식 정규화
    - 기계적인 방식으로 해결 가능: a, b, c
    -  기계적인 방식으로 해결 **불**가능: d, e, f

## 9.1.8 파일 시스템 링크 순환
- symlink는 순환을 야기할 수 있다.
- 사례:
    - 문제 없는 경우:
    ```mermaid
    flowchart TD;
        root['/'] --> c1[index.html];
        root --> c2[subdir];
        c2 --> c3[index.html];
        c2 --> c4[logo.gif];
    ```
    - symlink가 순환을 야기하는 경우:
    ```mermaid
    flowchart TD;
        root['/'] --> c1[index.html];
        root --> c2[subdir];
        c2 -.-> root;
    ```
    index.html 파일 안에는 subdir/index.html을 가리키는 링크가 있다. 이 링크가 순환을 일으킨다.
    
## 9.1.9 동적 가상 웹 공간
- 악의적으로 로봇을 순환에 빠뜨리는 예시:
    - URL이 게이트웨이 애플리케이션을 가리킨다: G/W는 가상의 링크를 지닌 문서를 응답으로 만들어 내며, 이 가상의 링크는 또 다른 가상의 링크를 포함한 문서를 만들어 내는 재귀적 방법으로 순환에 빠뜨린다.

## 9.1.10 루프와 중복 피하기
- 순환을 피하기 위한 방법들:
    - URL 정규화([[9장 웹 로봇#9 1 7 URL 정규화하기|9.1.7]])
    - 너비 우선 크롤링:
        - 깊이 우선 크롤링 약점:
            - 순환에 빠질 수 있다.
        - 너비 우선 크롤링 장점:
            - 순환의 영향을 최소화: 순환에 들어가도 다른 웹 사이트들을 받아올 수 있다.
    - throttling[^2]\:
        - 웹 사이트에서 일정 시간 동안 가져올 수 있는 페이지 숫자 제한
    - URL 크기 제한
        - 보통 1KB를 넘는 URL을 로봇이 거부하도록 설정 가능
        - 단점: 가져올 수 없는 컨텐츠가 생길 수 있다.
        - 장점: 특정 크기에 도달할 때마다 로그를 남겨 사이트에서 무슨 일이 벌어지는지 확인 가능.
    - URL 블랙리스트
        - 순환을 만들거나, 로봇의 행동을 가로막는 사이트 목록을 만든다.
        - 이미 알려진 곳들을 나열한 목록들을 재활용할 수 있다.
    - 패턴 발견
        - 순환을 야기하는 URL은 일정한 패턴이 나타날 수 있다. 이러한 패턴이 나타나는 URL의 크롤링을 막음으로서 순환에 빠지는 일을 막는다.
        - /subdir/images/subdir/images/...
    - 콘텐츠 지문(fingerprint)
        - 체크섬 얻기: 페이지 콘텐츠에서 몇 바이트를 가져와 체크섬을 계산한다. 이 체크섬을 이미 본 적이 있다면 이미 수집된 페이지이므로 크롤링에서 제외한다.
        - 체크섬 함수 조건: 내용이 다른데 체크섬 결과는 같을 확률이 희박해야 한다. MD5도 많이 쓰인다.
        - 페이지 내 체크섬에 쓰이기 적당하지 않은 부분: 임베딩된 링크, 날짜나 방문 횟수 등 동적으로 바뀔 수 있는 부분
    - 사람의 모니터링:
        - 특이 사항을 발견하면 진단과 로깅을 실시하도록 로봇을 설계
- 커스터마이징 크롤러의 장점:
    - 자원에 미치는 영향(네트워크 대역폭 등)을 조절 가능

[^2]: 원래 의미는 '조이다', '가로막다' 등이며 파이프에서 유량을 제한한다는 의미도 있다.

# 9.2 로봇의 HTTP
- 로봇 또한 HTTP 위의 애플리케이션이므로 HTTP 명세를 따라야 한다.
- 많은 로봇은 특수 목적을 지닌 애플리케이션이므로 최소한의 명세만 따르려는 경향이 있다.

## 9.2.1 요청 헤더 식별하기
- 로봇이 구현해야 할 최소한의 헤더:
    - 신원 식별 헤더: User-Agent
    - 로봇이 어디서 왔는지 알려주는 헤더: From, Referer
    - 로봇의 능력을 나타내는 헤더: Accept

## 9.2.2 가상 호스팅
- 가상 호스팅은 흔하므로 Host 헤더는 지원토록 한다. 그렇지 않으면 잘못된 콘텐츠를 요청하게 된다. 더욱 큰 문제는 받아온 문서가 잘못된 호스트에서 나온 것임을 모른다는 것이다.

## 9.2.3 조건부 요청
- 로봇이 수집해야 할 문서량이 방대하므로 이를 줄일 수 있다면 좋다.
- 방법: 조건부 HTTP 요청
    - 캐시의 유효성을 검사하는 방법([[7장 캐시]])과 유사.

## 9.2.4 응답 다루기
- GET 이외 경우 응답을 다룰 줄 알아야 하는 경우:
    - 조건부 요청 같은 고급 기능 사용 시
    - 웹 탐색이나 서버와 깊은 상호작용을 원할 시
- 이러한 응답에는:
    - 상태 코드:
        - 200, 404 같은 코드는 기본적으로 알아야 하며, 그 외 필요한 코드를 이해할 수 있도록 구현해야 한다.
        - 모든 서버가 적절한 에러 코드를 돌려주진 않는다는 결함도 예상해야 한다. 200 OK를 에러 응답을 나타내는데 쓰는 서버가 있을 수도 있다.
    - 엔터티:
        - `http-equiv` 태그는 콘텐츠 저자가 서버가 제공하는 헤더를 덮어쓰기 위해 작성한다. 로봇 개발 시 HTML 문서를 열어보도록 해야 할 수도 있다.

## 9.2.5 User-Agent 타켓팅
- 클라이언트별 맞춤 콘텐츠 제공 등 로봇 개발자 외 사이트 개발자도 로봇에 대응하도록 노력해야 한다:

# 9.3 부적절하게 동작하는 로봇들
- 로봇이 흔히 저지르는 실수:
    - 폭주하는 로봇:
        - 논리적인 에러를 내포하거나 순환에 빠진 경우 막대한 양의 요청을 보내 서버를 마비시킨다.
        - 폭주 상황에 빠졌다면 이를 빠져나와 문제를 해결할 대비책을 마련해 둬야 한다.
    - 오래된 URL:
        - 원인: 존재하지 않는 URL
        - 결과: 쓸모 없는 에러 로그, 서버 부하
    - 길고 잘못된 URL
        - 원인: 순환, 프로그래밍 오류
        - 영향:
            - 서버 처리 능력 저하, 고장
            - 보기 싫은 접근 로그
    - 호기심이 지나친 로봇
        - 호기심이 지나치다: 사적인 데이터로 이어지는 URL 획득.
        - 원인: URL은 존재하지 않지만 디렉터리 URL이 반환한 목록에서 콘텐츠를 가져오는 경우 등.
        - 해결책: 로봇 개발 시 이런 사실을 인지하고 의도하지 않았을 콘텐츠를 무시하거나, 가져왔다면 제거하는 매커니즘을 구현
    - 동적 게이트웨이 접근
        - 게이트웨이 애플리케이션 콘텐츠는 정적 콘텐츠보다 비용이 높으며, 이런 콘텐츠는 순환에 빠지기도 쉽다.

# 9.4 로봇 차단하기
- Robots Exclusion Standard: 로봇에 의한 웹 사이트 접근 문제를 해결하기 위한 매커니즘 제안
    - 웹 마스터가 로봇의 동작을 제어
    - 로봇이 웹 사이트를 수집해야 할지 미리 알려줌
    - 대부분은 robots.txt라고 불림.
    - 아이디어:
        1. docroot/robots.txt 마련: 어떤 로봇이 서버 어디에 접근 가능/불가능한지 명시
        2. 표준을 따르는 로봇이 robots.txt를 요청하고 이 파일을 검사.

## 9.4.1 로봇 차단 표준
- 불완전하지만 그래도 널리 쓰임
- 표준: 대부분 1.0을 사용.
| 버전 | 이름 및 설명 | 날짜 |
| -- | -- | -- |
| 0.0 | Disallow를 지원하는 오리지널 메커니즘 | 94.06 |
| 1.0 | Allow 지시자 추가 | 96.11 |
| 2.0 | 차단을 위한 확장 표준정규식과 타이밍 정보 추가 | 96.11|

## 9.4.2 웹 사이트와 robots.txt 파일들
- 웹 사이트가 robots.txt를 마련했다면 로봇은 반드시 이 파일을 요청해야 함
- 호스트명과 포트 번호에 정의되는 한 사이트 전체를 통틀어 단 하나의 robots.txt만이 (docroot에) 존재해야 한다.
- 이 robots.txt에서 웹 사이트 내 모든 콘텐츠에 대한 차단 여부를 결정해야 한다.
- 로봇이 robots.txt를 GET 요청:
    - 있음: text/plian MIME 타입 200 OK로 응답
    - 없음:
        - 404 Not Found 응답. 로봇 접근을 서버가 제한하지 않는다고 간주.
        - 401 혹은 403 응답: 사이트 자체 완전 접근 금지.
        - 503: 리소스 검색 연기
        - 3XX: 리소스 발견 시까지 리다이렉트를 따라간다.
- 방문 로봇의 책임:
    - From, User-Agent 헤더 등으로 신원 정보를 남김
    - 웹 관리자가 로봇에 대해 문의할 수 있도록 연락처를 제공해야 함.
    ```
    GET /robots.txt HTTP/1.0
    Host: www.joes-hardware.com
    User-Agent: Slurp/2.0
    Date: Wed Oct 3 20:22:48 EST 2001
    From: admin@robot.com
    ```

## 9.4.3 robots.txt 파일 포맷
```
User-Agent: slurp
User-Agent: webcrawler
Disallow: /private

User-Agent: * # 모든 에이전트
Disallow:  # 모든 리소스
```
- 레코드: 특정 로봇 집합에 대한 차단 규칙의 집합. 위에는 2 레코드가 있다.
    - 각 레코드는 규칙 줄들로 이루어진다.
    - 레코드는 공백 줄이나 파일 끝으로 구분된다.
    - User-Agent로 시작하며 이어서 Disallow, Allow가 온다.
- 파일 각 줄의 종류: 빈 줄, 주석 줄, 규칙 줄
    - 빈 줄: 레코드 구분
    - 주석 줄: `#`부터 줄바꿈까지 내용은 로봇에 의해 모두 무시된다.
    - 규칙 줄: `<Field>:<Value>`
        - User-Agent 필드: 레코드가 적용될 에이전트를 명시. 로봇은 자신의 요청 User-Agent 헤더에 이름을 적는다. 적용 레코드 발견 순서는 다음과 같다. 대소문자 구분하지 않는다:
            - '규칙 줄 내 이름 ⊂ 로봇 실제 이름'인 레코드 중 첫번째
            - `User-Agent: * `중 첫번째
        - Disallow 필드: 허용하지 않는 URL
        - Allow 필드: 허용하는 URL
            - 모든 Allow, Disallow 중 첫번째로 적용되는 것을 사용한다.
            - 빈 문자열은 모든 문자열의 표현이다.
            - 대소문자를 구분한다.
            - 빗금(%2F)을 제외한 모든 이스케이프 문자는 비교 전에 변환된다.
            - 요청할 URL을 규칙 필드에 지정된 URL 길이만큼 비교한다.
        ![[IMG_4101.jpg|512]]

## 9.4.4 그 외 알아둘 점
- 필드 종류가 늘어날 수 있으나, 로봇은 자신이 이해하지 못하는 필드는 무시해야 한다.
- 한 줄을 여러 줄로 나눠서는 안된다.
- 0.0은 Allow를 지원하지 않는다.

## 9.4.5 robots.txt 캐싱과 만료
- 로봇이 매번 robots.txt를 요청하는 일을 막고자 캐싱을 사용한다.
    - 표준 HTTP 캐시 제어 매커니즘을 서버와 로봇 양측에 적용한다.
    - Cache-Control, Expires 헤더로 제어.

## 9.4.6 로봇 차단 Perl 코드
WWW::RobustRules 모듈로 robots.txt를 파싱하여 접근 여부를 확인할 수 있다.
- www::RobotRules API의 주요 메서드:
    - RobotRules 객체 만들기: `$rules WWW::RobotRules->new($robot_name);`
    - robots.txt 파일 로드하기: `$rules->parse($url, Scontent, $fresh_until);`
    - 사이트 URL을 가져올 수 있는지 검사하기: `$can_fetch = Srules->allowed($url);`
- WWW::RobotRules 예시 프로그램.
    ```perl
    require Www::RobotRules;
    
    #로봇의 이름을 "SuperRobot"으로 하여 RobotRules 객체를 생성한다. 
    my $robotsrules = new WWW::RobotRules 'SuperRobot/1.0'; 
    use LWP::Simple qw(get); 
    
    #죠의 하드웨어에서 robots.txt 파일을 가져와서 파싱하고 규칙을 더한다. 
    $url = "http://www.joes-hardware.com/robots.txt"; 
    my Srobots_txt = get $url;
    $robotsrules->parse($url, Srobots_txt);
    
    #메리의 골동품상점에서 robots.txt 파일을 가져와서 파싱하고 규칙을 더한다. 
    Surl="http://www.marys-antiques.com/robots.txt";
    
    my $robots_txt = get $url;
    $robotsrules->parse($url, $robots_txt);
    
    #이제 RobotRules는 여러 다른 사이트에서 가져온 로봇 차단 규칙을 포함하고 있다. 
    #이규칙들은 모두 따로 유지된다. 이제 우리는 RobotRule를 이용해서 여러 URL에 
    #대해 로봇의 접근이 허용되어 있는지 검사할 수 있다. 
    if($robotsrules->allowed($some_target_url))
        $c = get $url;
        # ...
    }
    ```
- www.marys-antiques.com 위한 가상의 robots.txt 파일이다.
    ```
    ################################################# 
    # 이것은 메리의 골동품 상점 웹 사이트의 robots.txt 파일이다. 
    #################################################
    
    #수지의 로봇은 모든 동적 URL을 이해할 수 없으므로, 그것들에는 접근할
    # 수 없게 한다. 그리고 메리가 수지를 위해 예약해둔 작은 영역을 제외한
    #모든 사적 데이터에도 접근할 수 없게 한다.
    User-Agent: Suzy-Spider
    Disallow: /dynamic
    Allow: /private/suzy-stuff
    Disallow: /private
    
    #가구 탐색 로봇은 특별히 메리의 골동품 상점에서도 가구 재고관리
    #프로그램을 이해할 수 있도록 설계되었으므로 그에 대한 리소스들을 
    #크롤링할 수 있게 해주되, 그 외 다른 동적 리소스와 사적 데이터에는 
    #접근할 수 없게 한다.
    User-Agent: Furniture-Finder
    Allow: /dynamic/check-inventory
    Disallow: /dynamic
    Disallow: /private
    
    #그외 나머지는 동적 게이트웨이와 사적 데이터에 접근할 수 없게 한다.
    User-Agent: *
    Disallow: /dynamic
    Disallow: /private
    ```



## 9.4.7 HTML 로봇 제어 META 태그
- robots.txt의 단점: 콘텐츠 작성자에게 제어권이 없다.
- 이를 해결하기 위한 방안: 로봇 제어 태그
    - 형식: `<META NAME="ROBOTS" CONTENT=directive-list>`
    - 위치: 모든 META 태그는 `<head></head>` 안에 작성해야 한다.
    - 값은 대소문자를 구분하지 않는다.
    - directive-list에 들어갈 수 있는 값:
        - NOINDEX: 페이지를 처리하지 말고 무시하라
        - NOFOLLOW: 페이지 내 링크된 문서를 크롤링하지 말 것
        - NOARCHIVE: 로봇이 이 문서의 캐시 사본을 만들어선 안된다.
        - ALL: INDEX, FOLLOW를 한번에
        - NONE: NOINDEX, NOFOLLOW를 한번에.
- 타 검색엔진 META 태그:
    - name=DESCRIPTION: 짧은 요약 텍스트를 값으로 지닌다. 
    - name=KEYWORDS: 쉼표로 구분되는 사이트 핵심 키워드 나열.
    - name=REVISIT-AFTER:`<n> days`로 지정 일 수 이후 재방문하라고 알려준다.

# 9.5 로봇 에티켓
- 웹 로봇을 만드는 사람을 위한 [가이드라인(by Martijn Koster)](http://www.robotstxt.org/wc/guidelines.html):
![[IMG_4103.jpg|512]]
![[IMG_4104.jpg|512]]
![[IMG_4105.jpg|512]]

# 9.6 검색엔진
- 웹 로봇이 검색엔진에 주는 도움: 어떤 문서에 어떤 단어가 존재하는지 색인을 만들어 준다.

## 9.6.1 넓게 생각하라
- 초창기 검색엔진: 문서의 위치를 알아내는 것을 돕는 DB
- 현대 검색엔진: 병렬 크롤러 사용

## 9.6.2 현대적인 검색 엔진의 아키텍처
- full-text indexes: 검색엔진이 생성하는 복잡한 로컬 DB로서 카드 카탈로그(캐비넷 서랍마다 든 카드 다발)처럼 작동.
- 클로링된 페이지들은 이 풀 텍스트 색인에 담긴다.
- 사용자들은 G/W를 통해 풀 텍스트 색인으로 질의를 보낸다.

## 9.6.3 풀 텍스트 색인
- full-text indexes는 단어 하나와 그 단어를 포함하는 문서들의 목록이 나열된 표라고 보면 된다. 
| 단어 | 웹페이지 |
| -- | -- |
| best | A, C |
| buy | B |

## 9.6.4 질의 보내기
- 사용자가 질의를 웹 검색엔진 G/W로 보내는 과정:
    1. HTML 폼을 채워 요청을 보낸다.
    2. G/W는 요청에서 검색 질의를 추출한다.
    3. 추출한 질의를 full-text index 질의를 위한 표현식으로 변환
    4. 검색어에 대응하는 웹페이지 목록을 웹 서버는 클라이언트를 위한 HTML 문서로 만들어 반환한다. (G/W가 담당할 수도 있다.)

## 9.6.5 검색 결과를 정렬하고 보여주기
- 관련도 랭킹(relevancy ranking): 질의에 대응하는 웹페이지가 다수 존재할 수 있기에, 이들 간 순위를 매기는 알고리즘.
- 수집 과정에서 생성된 통계 데이터를 활용: 문서를 가리키는 링크의 개수가 많을수록 랭킹에 반영.

## 9.6.6 스푸핑
- 높은 순위를 차지하고자 웹사이트에 속임수들를 심기도 한다.
    - 관련이 없는 수많은 키워드 나열
    - 특정 단어에 대한 가짜 페이지 생성 게이트웨이 애플리케이션 사용
